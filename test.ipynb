{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         description  \\\n",
      "0  Nous recherchons un chef de projet (h/f) pour ...   \n",
      "1  Engineer (m/f/d) required for international pr...   \n",
      "2            Verkäufer (m/w) für unser Team gesucht.   \n",
      "3  Looking for a Senior Developer (h/f) with 5 ye...   \n",
      "4  We need a Data Scientist h/f in our Paris office.   \n",
      "\n",
      "                        job_title  \n",
      "0   recherchons un chef de projet  \n",
      "1                        Engineer  \n",
      "2                       Verkäufer  \n",
      "3  Looking for a Senior Developer  \n",
      "4        We need a Data Scientist  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Exemple de DataFrame avec des descriptions d'offres d'emploi\n",
    "data = {'description': [\n",
    "    \"Nous recherchons un chef de projet (h/f) pour notre entreprise.\",\n",
    "    \"Engineer (m/f/d) required for international project.\",\n",
    "    \"Verkäufer (m/w) für unser Team gesucht.\",\n",
    "    \"Looking for a Senior Developer (h/f) with 5 years of experience.\",\n",
    "    \"We need a Data Scientist h/f in our Paris office.\"\n",
    "]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Liste des abréviations à détecter, incluant les parenthèses\n",
    "abbreviations = r\"\\(?h/f\\)?|\\(?m/f\\)?|\\(?m/w\\)?|\\(?m/v\\)?|\\(?m/k\\)?|\\(?m/n\\)?|\\(?m/ž\\)?|\\(?f/n\\)?|\\(?b/f\\)?|\\(?άν/γυν\\)?|\\(?м/ж\\)?\"\n",
    "\n",
    "# Fonction pour extraire les 5 mots précédant l'abréviation\n",
    "def extract_job_title(line):\n",
    "    # Regex pour capturer les 5 mots précédant l'abréviation\n",
    "    match = re.search(r'(\\b\\w+\\b[\\s,]*){1,5}(?=\\s*(' + abbreviations + '))', line, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group().strip()\n",
    "    return None\n",
    "\n",
    "# Appliquer la fonction à la colonne 'description' pour extraire les libellés de poste\n",
    "df['job_title'] = df['description'].apply(extract_job_title)\n",
    "\n",
    "# Afficher le DataFrame avec les résultats\n",
    "print(df[['description', 'job_title']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npq.write_to_dataset(\\n    pa.Table.from_pandas(data),\\n    root_path=URL_DATASET_WITH_LANG,\\n    partition_cols=[\"lang\"],\\n    basename_template=\"part-{i}.parquet\",\\n    existing_data_behavior=\"overwrite_or_ignore\",\\n    filesystem=fs,\\n)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import s3fs\n",
    "import os\n",
    "\n",
    "def get_file_system() -> s3fs.S3FileSystem:\n",
    "    \"\"\"\n",
    "    Return the s3 file system.\n",
    "    \"\"\"\n",
    "    return s3fs.S3FileSystem(\n",
    "        client_kwargs={\"endpoint_url\": f\"https://{os.environ['AWS_S3_ENDPOINT']}\"},\n",
    "        key=\"59GW2Q1OXSLD5ER6S44I\",\n",
    "        secret=\"P7Dqk6KODsEQO+PBAVlus2sgiY+eJ0DSDDmAIlmE\",\n",
    "        token=\"eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NLZXkiOiI1OUdXMlExT1hTTEQ1RVI2UzQ0SSIsImFsbG93ZWQtb3JpZ2lucyI6WyIqIl0sImF1ZCI6WyJtaW5pby1kYXRhbm9kZSIsIm9ueXhpYSJdLCJhdXRoX3RpbWUiOjE3MjY2NzQyNjMsImF6cCI6Im9ueXhpYSIsImVtYWlsIjoieXZlcy1sYXVyZW50LmJlbmljaG91QGluc2VlLmZyIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImV4cCI6MTcyNzcwMTU5NSwiZmFtaWx5X25hbWUiOiJCZW5pY2hvdSIsImdpdmVuX25hbWUiOiJZdmVzLUxhdXJlbnQiLCJncm91cHMiOlsiVVNFUl9PTllYSUEiLCJhZGRvayIsImFkcmVzc2VzLXJldSIsImFwZSIsImNvZGlmaWNhdGlvbi1wY3MiLCJkZWR1cC1vamEiLCJlc2Etbm93Y2FzdGluZyIsImZ1bmF0aG9uIiwiaGFja2F0aG9uLW50dHMtMjAyMyIsInByZWRpY2F0IiwicmVsZXZhbmMiLCJzaWduZXMtZGUtdmllIiwic2l0ZXNzcGxhYiIsInNzcGxhYiJdLCJpYXQiOjE3MjcwOTY3OTQsImlzcyI6Imh0dHBzOi8vYXV0aC5sYWIuc3NwY2xvdWQuZnIvYXV0aC9yZWFsbXMvc3NwY2xvdWQiLCJqdGkiOiJlNWYxMjkxYy04NjZhLTQ2NmItYjFlZi05ZmVmNmJkNWNiYjYiLCJsb2NhbGUiOiJlbiIsIm5hbWUiOiJZdmVzLUxhdXJlbnQgQmVuaWNob3UiLCJwb2xpY3kiOiJzdHNvbmx5IiwicHJlZmVycmVkX3VzZXJuYW1lIjoiZG9oYW9xIiwicm9sZSI6InJvbGUxIiwic2NvcGUiOiJvcGVuaWQgcHJvZmlsZSBncm91cHMgZW1haWwiLCJzaWQiOiJmOGY5MzE5MC1kODc0LTRmMmQtODY3NC02OTkxZmQ5NWI5NDYiLCJzdWIiOiIxYmFlYjY3MC01YmI4LTRhZDEtOGQzMS1lNjM5ZDllYmRjZGIiLCJ0eXAiOiJCZWFyZXIifQ.QZzwcIyB7fScKsvrZ72JlOCxWY7cp4cBfqJPtLF-rWzUHjw5pKya7TLArV9B91ntWr_x6YxlyAeGNUrMtZeDyw\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from src.constants.paths import URL_DATASET, URL_DATASET_WITH_LANG\n",
    "from src.detect_lang.detect import detect_language, process_data_lang_detec\n",
    "#from src.utils.data import get_file_system\n",
    "from src.utils.mapping import id_881693105_desc\n",
    "\n",
    "DESC_CUTOFF_SIZE = 500\n",
    "\n",
    "fs = get_file_system()\n",
    "eol_regex = re.compile(r\"\\r|\\n\")\n",
    "\n",
    "with fs.open(URL_DATASET) as f:\n",
    "    data = pd.read_csv(f, dtype=str)\n",
    "\n",
    "# Manually set specific description for one ID\n",
    "data.loc[data[\"id\"] == \"881693105\", \"description\"] = id_881693105_desc\n",
    "\n",
    "# Process the dataset with cleaning and job title extraction\n",
    "data = process_data_lang_detec(data)\n",
    "\n",
    "# Detect language and score\n",
    "data[[\"lang\", \"score\"]] = (\n",
    "    data[\"description_clean\"]\n",
    "    .str.replace(eol_regex, \" \", regex=True)\n",
    "    .apply(detect_language)\n",
    "    .apply(pd.Series)\n",
    ")\n",
    "\n",
    "# Truncate the description\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=DESC_CUTOFF_SIZE,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \";\", \":\", \",\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    splitted_text = text_splitter.split_text(row.description_clean)\n",
    "    text_truncated = \"\"\n",
    "    i = 0\n",
    "    while (len(text_truncated) < DESC_CUTOFF_SIZE) and (i < len(splitted_text)):\n",
    "        text_truncated += f\" {splitted_text[i]}\"\n",
    "        i += 1\n",
    "    data.loc[idx, \"description_truncated\"] = text_truncated\n",
    "\"\"\"\n",
    "pq.write_to_dataset(\n",
    "    pa.Table.from_pandas(data),\n",
    "    root_path=URL_DATASET_WITH_LANG,\n",
    "    partition_cols=[\"lang\"],\n",
    "    basename_template=\"part-{i}.parquet\",\n",
    "    existing_data_behavior=\"overwrite_or_ignore\",\n",
    "    filesystem=fs,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# # Fonction pour extraire les 5 mots précédant l'abréviation\n",
    "# def extract_job_title(line):\n",
    "#     # Regex pour capturer les 5 mots précédant l'abréviation\n",
    "#     match = re.search(r\"(\\b\\w+\\b[\\s,]*){1,5}(?=\\s*(\" + abbreviations + \"))\", line, re.IGNORECASE)\n",
    "#     if match:\n",
    "#         return match.group().strip()\n",
    "#     return None\n",
    "\n",
    "# # Appliquer la fonction extract_job_title aux colonne 'description' et 'title' pour extraire les libellés de poste\n",
    "# data[\"description_job_title\"] = data[\"description\"].apply(extract_job_title)\n",
    "# data[\"title_job_title\"] = data[\"title\"].apply(extract_job_title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en' 'sv' 'bg' 'es' 'sk' 'lv' 'sl' 'it' 'nl' 'lt' 'cs' 'et' 'hu' 'pl'\n",
      " 'el' 'de' 'ro' 'fr' 'pt' 'hr' 'fi' 'ru' 'da' 'sh' 'ca' 'ja' 'gl' 'sr']\n"
     ]
    }
   ],
   "source": [
    "print(data['lang'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          score lang\n",
      "128    0.256520   sl\n",
      "133    0.306960   en\n",
      "224    0.291793   nl\n",
      "443    0.359631   hr\n",
      "475    0.285939   sl\n",
      "...         ...  ...\n",
      "25207  0.325047   ro\n",
      "25352  0.358309   en\n",
      "25356  0.255807   en\n",
      "25409  0.171804   en\n",
      "25556  0.218276   sl\n",
      "\n",
      "[289 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data[['score','lang']][data['score']<0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lang'] = data.apply(lambda row: 'un' if row['score'] < 0.4 else row['lang'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128      un\n",
      "133      un\n",
      "224      un\n",
      "275      un\n",
      "443      un\n",
      "         ..\n",
      "25207    un\n",
      "25352    un\n",
      "25356    un\n",
      "25409    un\n",
      "25556    un\n",
      "Name: lang, Length: 313, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['lang'][data['lang']=='un'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.mapping import lang_mapping\n",
    "#country_map = lang_mapping.loc[lang_mapping[\"lang_iso_2\"].isin(list_country)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     en\n",
      "1     de\n",
      "2     bg\n",
      "3     hr\n",
      "4     da\n",
      "5     es\n",
      "6     et\n",
      "7     fi\n",
      "8     fr\n",
      "9     el\n",
      "10    hu\n",
      "11    ga\n",
      "12    it\n",
      "13    lv\n",
      "14    lt\n",
      "15    mt\n",
      "16    nl\n",
      "17    pl\n",
      "18    pt\n",
      "19    ro\n",
      "20    sk\n",
      "21    sl\n",
      "22    sv\n",
      "23    cs\n",
      "24    un\n",
      "Name: lang_iso_2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(lang_mapping[\"lang_iso_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.mapping import lang_mapping\n",
    "data['lang'] = data['lang'].where(data['lang'].isin(lang_mapping[\"lang_iso_2\"]), 'un')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        True\n",
      "1        True\n",
      "2        True\n",
      "3        True\n",
      "4        True\n",
      "         ... \n",
      "25660    True\n",
      "25661    True\n",
      "25662    True\n",
      "25663    True\n",
      "25664    True\n",
      "Name: lang, Length: 25665, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(data['lang'].isin(lang_mapping[\"lang_iso_2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
